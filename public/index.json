[{"authors":["admin"],"categories":null,"content":"Greetings! I\u0026rsquo;m a graduate student at UC Davis. My study focuses on completing functional annotation of the horse genome. I have a strong interest of applying genomics tools to non-model organisms to improve health care of companion and agricultral animals, as well as to enhance wildlife preservation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Greetings! I\u0026rsquo;m a graduate student at UC Davis. My study focuses on completing functional annotation of the horse genome. I have a strong interest of applying genomics tools to non-model organisms to improve health care of companion and agricultral animals, as well as to enhance wildlife preservation.","tags":null,"title":"Sichong Peng","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536476400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536476400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["Linux"],"content":" I was assisting a bioinformatics workshop for FAANG group at Plant and Animal Genome (PAG) and I ran into an interesting question with an example provided by one of the instructors on the topic of Linux output redirecting. Take a look at this simplified example script print_stderr.sh:\n#!/usr/bin/bash echo \u0026quot;first line of stderr\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;second line of stderr\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;third line of stderr\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;A line of stdout\u0026quot;  Now if we redirect the stderr of this script to a file, instead of all three lines of stderr messages, we only see the last line!\n$ ./print_stderr.sh 2\u0026gt; err.log $ cat err.log third line of stderr  Why is this happening??\nNow remember, when we redirect output of a command to stderr, we typically do it like so:\ncommand \u0026gt;\u0026amp;2  What if we modify the script according to this?\n#!/usr/bin/bash echo \u0026quot;first line of stderr\u0026quot; \u0026gt;\u0026amp;2 echo \u0026quot;second line of stderr\u0026quot; \u0026gt;\u0026amp;2 echo \u0026quot;third line of stderr\u0026quot; \u0026gt;\u0026amp;2 echo \u0026quot;A line of stdout\u0026quot;  $ ./print_stderr.sh 2\u0026gt; err.log $ cat err.log first line of stderr second line of stderr third line of stderr  Now it works as we intended it to! So what\u0026rsquo;s the difference?\nNotice that I quoted \u0026ldquo;redirect\u0026rdquo; above. Because what \u0026ldquo;\u0026gt;\u0026amp;\u0026rdquo; idiom really does is duplicating, not redirecting. And that is the key to understand this interesting behaviour here.\nTo understand the difference, we first need to go over the three special file descriptors and how Linux handles its STDIO.\nWhat is a file descriptor When a process starts, the kernel indexes all the files it requires and associate them with what are known as \u0026ldquo;file descriptors\u0026rdquo;. In Linux systems, the first three files descriptors (0, 1, and 2) are by default STDIN, STDOUT, and STDERR.\nYou may have heard that on a linux system, \u0026ldquo;everything is a file\u0026rdquo;. And that is also true with I/O. STDIN, STDOUT, and STDERR all point to files on a system. When a process runs, it reads data from STDIN and writes to STDOUT and STDERR. Whether that STDOUT then goes to a console, a file, or something else is handled by the kernel. This level of absraction makes Linux particularly powerful and portable.\nHow does redirecting work? So when we write a command like this in a bash shell:\ncommand 2\u0026gt; err.log  We are essentially telling the shell to write STDERR to a file err.log instead of its default place.\nNow where would that default place be?\nIn an interactive shell, all three STDIO streams by default point to a device file. Many shells, including Bash, implement symlink files /dev/stdin, /dev/stdout, and /dev/stderr. These links point to /proc/$pid/fd/0, /proc/$pid/fd/1, /proc/$pid/fd/2, respectively.\nIf you take a look at these files, you can see that they all point to a device file:\n$ ls -lah /proc/self/fd/[0,1,2] lrwx------ 1 sichong sichong 64 Jan 9 12:40 /proc/self/fd/0 -\u0026gt; /dev/pts/2 lrwx------ 1 sichong sichong 64 Jan 9 12:40 /proc/self/fd/1 -\u0026gt; /dev/pts/2 lrwx------ 1 sichong sichong 64 Jan 9 12:40 /proc/self/fd/2 -\u0026gt; /dev/pts/2  Not to go into too much detail, /dev/pts/2 is a pseudo-terminal created by your current interactive shell. It allows the shell to read your keyboard input, as well as to write to your screen. Try writing something to this device file:\n$ echo \u0026quot;Hello\u0026quot; \u0026gt; /dev/pts/2 Hello  It immediately shows up on your screen!\nNow you would ask, shouldn\u0026rsquo;t every process have its own STDIO streams? If they all end up to the same place, how does the kernel know where each stream go to?\nPrecisely! Every process gets its own set of file descriptors (stroed in /proc/$pid/fd where $pid is the process id). Now you probably know that when we run a script in an interactive shell, the shell calls a child process to run the script. So where do the STDIO streams of the child process point to?\nIt turns out that by default, the child process inherits from the parent process its STDIO targets. We can verify this by running a script like this(show_file_descriptors.sh):\n#!/usr/bin/bash filan -s  (You may need to install this program with sudo apt install socat)\nAnd you should see an output that looks like this:\n$ ./show_file_descriptors.sh 0 tty /dev/pts/2 1 tty /dev/pts/2 2 tty /dev/pts/2  This tells us that our file descriptors all point to /dev/pts/2 (same device file as our current shell session!) which is connected to a tty device.\nNow what happens if I redirect the STDERR of this file to a different file?\n$ ./show_file_descriptors.sh 2\u0026gt; err.log 0 tty /dev/pts/2 1 tty /dev/pts/2 2 file /home/sichong/err.log  Now it\u0026rsquo;s different! Our file descriptor 2 now points to the file err.log!\nTo summarize, an interactive shell by default have all three of its STDIO streams (STDIN, STDOUT, STDERR) point to a same device file in /dev/pts directory. All child processes spawned by this shell session by dafault inherit the same file descriptors. (Hence when we run a program in a shell session it can take input from keyboard and we see its output on a screen, unless handled differently by the program.) But when we redirect input or output of a program, the spawned child process gets new file descriptors that point to redirected files.\nSo, what happened? At this point, it is probably clear to you already why the script we used at the workshop didn\u0026rsquo;t work as intended: The child process running the script gets err.log as its STDERR. This means that the /dev/stderr link points to /proc/self/fd/2 which points to err.log instead of /dev/pts/2! So now when you redirect command of echo to /dev/stderr in the script, you\u0026rsquo;re actually redirecting it to a file err.log. And since we are using \u0026gt; instead of \u0026gt;\u0026gt; to truncate the file, we only get whatever gets put in last.\nTo test our hypothesis, we can modify our script to use \u0026gt;\u0026gt; instead of \u0026gt; after each echo command. If our theory stands, this should make our script behave!\nprint_stderr.sh:\n#!/usr/bin/bash echo \u0026quot;first line of stderr\u0026quot; \u0026gt;\u0026gt; /dev/stderr echo \u0026quot;second line of stderr\u0026quot; \u0026gt;\u0026gt; /dev/stderr echo \u0026quot;third line of stderr\u0026quot; \u0026gt;\u0026gt; /dev/stderr echo \u0026quot;A line of stdout\u0026quot;  Now let\u0026rsquo;s try it again:\n$ ./print_stderr.sh 2\u0026gt; err.log $ cat err.log first line of stderr second line of stderr third line of stderr  Tada!\nBut why is \u0026gt;\u0026amp; different then? To understand this, we need to dive deeper into how Linux systems handle redirecting. We are gonna use a program called strace to follow the system and see how it handles file descriptor at each step. Try a simple command:\n$ strace -f -e trace=openat,write,dup2 sh -c 'ls \u0026gt; out'  We would get output like this:\nopenat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;out\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 dup2(3, 1) = 1 strace: Process 10444 attached [pid 10444] openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libselinux.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64-linux-gnu/libpcre2-8.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libpthread.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/proc/filesystems\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10444] openat(AT_FDCWD, \u0026quot;.\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 [pid 10444] write(1, \u0026quot;anaconda3\\nandroid-studio\\nApps\\nbi\u0026quot;..., 268) = 268 [pid 10444] +++ exited with 0 +++ --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=10444, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- dup2(10, 1) = 1 +++ exited with 0 +++  Some important lines to notice:\nopenat(AT_FDCWD, \u0026quot;out\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3  This line indicates that the kernel opens file out and assignes it a file descriptor 3. Notice the O_TRUNC keyword here. This is where truncating happens with \u0026gt; redirecting: the system tries to open the target file, and truncate it if not empty, before the command is even evaluated!\n On a side note, this is also why doing sort data.txt \u0026gt; data.txt could potentially get you fired :)\n After this step, the file is ready for writing and will take wahtever comes to it in sequential order, meaning no more truncating or overwriting will happen! (Unless another redirecting to the same file happens.)\ndup2(3, 1)  This line suggests that the kernel duplicates the value of file descriptor 3 (which points to out) to file descriptor 1 (STDOUT).\nNow at this point the STDOUT points to out instead of /dev/pts/2!\nstrace: Process 10444 attached  This process means that a child process is called to run ls\nwrite(1, \u0026quot;anaconda3\\nandroid-studio\\nApps\\nbi\u0026quot;..., 268) = 268  This line means that the output of ls is being written to file descriptor 1, which points to out.\nSo this is what happens under the hood when we tell our shell to redirect the output of ls to a file named out!\n What if instead of \u0026gt; we use \u0026gt;\u0026gt; to redirect output of ls?\n Now let\u0026rsquo;s make a script (redirect.sh):\n#!/usr/bin/bash echo \u0026quot;Hello,\u0026quot; echo \u0026quot;World!\u0026quot;  And let\u0026rsquo;s trace what happens when we redirect output of this script:\n$ strace -f -e trace=openat,write,dup2 sh -c './redirect.sh \u0026gt; out'  You should see output similar to this:\nopenat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;out\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 dup2(3, 1) = 1 strace: Process 10970 attached [pid 10970] openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libtinfo.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;/dev/tty\u0026quot;, O_RDWR|O_NONBLOCK) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\u0026quot;, O_RDONLY) = 3 [pid 10970] openat(AT_FDCWD, \u0026quot;./test\u0026quot;, O_RDONLY) = 3 [pid 10970] dup2(3, 255) = 255 [pid 10970] write(1, \u0026quot;Hello,\\n\u0026quot;, 7) = 7 [pid 10970] write(1, \u0026quot;World!\\n\u0026quot;, 7) = 7 [pid 10970] +++ exited with 0 +++ --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=10970, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- dup2(10, 1) = 1 +++ exited with 0 +++  This is very similar to what we see before. Notice that the child process does not attempt to open out file again! That is why ./redirect.sh \u0026gt; out will print\nHello, World!  to file out instead of only the last line!\nBut now let\u0026rsquo;s modify our script:\n#!/usr/bin/bash echo \u0026quot;Hello,\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;World!\u0026quot; \u0026gt; /dev/stderr  And redirect output of this script to a file:\n$ strace -f -e trace=openat,write,dup2 sh -c './redirect.sh 2\u0026gt; err'  Now you will see output like this:\nopenat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;err\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 dup2(3, 2) = 2 strace: Process 11106 attached [pid 11106] openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libtinfo.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;/dev/tty\u0026quot;, O_RDWR|O_NONBLOCK) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\u0026quot;, O_RDONLY) = 3 [pid 11106] openat(AT_FDCWD, \u0026quot;./test\u0026quot;, O_RDONLY) = 3 [pid 11106] dup2(3, 255) = 255 [pid 11106] openat(AT_FDCWD, \u0026quot;/dev/stderr\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 [pid 11106] dup2(3, 1) = 1 [pid 11106] write(1, \u0026quot;Hello,\\n\u0026quot;, 7) = 7 [pid 11106] dup2(10, 1) = 1 [pid 11106] openat(AT_FDCWD, \u0026quot;/dev/stderr\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 [pid 11106] dup2(3, 1) = 1 [pid 11106] write(1, \u0026quot;World!\\n\u0026quot;, 7) = 7 [pid 11106] dup2(10, 1) = 1 [pid 11106] +++ exited with 0 +++ --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=11106, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- dup2(10, 2) = 2 +++ exited with 0 +++  Notice that the same file err is opened and truncated three times here:\nopenat(AT_FDCWD, \u0026quot;err\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3  First it is opened by parent process and assigned file desccriptor 3\ndup2(3, 2)  Then in the parent process the value of file descriptor 3 is copied to 2 (STDERR). And then child process is called which inherits the parent\u0026rsquo;s STDERR (points to err at this point).\n[pid 11106] openat(AT_FDCWD, \u0026quot;/dev/stderr\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 [pid 11106] dup2(3, 1) = 1 [pid 11106] write(1, \u0026quot;Hello,\\n\u0026quot;, 7) = 7  And then the child process attempts to open the file it\u0026rsquo;s supposed to redirect to (/dev/stderr), truncates it, and assigns it a file descriptor 3 (separately stored from parent\u0026rsquo;s descriptors). Then it copies the value of file descriptor 3 to 1 (STDOUT). This completes the redirecting following echo command (\u0026gt; /dev/stderr). And finally it writes output of echo command to its STDOUT (which now points to err file).\nAnd the same process repeats with the second echo command, removing output in the err file by the first echo command.\nThis all makes sense, but why is \u0026gt;\u0026amp; different?\nWell let\u0026rsquo;s see what our system does when we use \u0026gt;\u0026amp;:\nBut now let\u0026rsquo;s modify our script:\n#!/usr/bin/bash echo \u0026quot;Hello,\u0026quot; \u0026gt;\u0026amp; 2 echo \u0026quot;World!\u0026quot; \u0026gt;\u0026amp; 2  And run it again like this:\n$ strace -f -e trace=openat,write,dup2 sh -c './redirect.sh 2\u0026gt; err'  Now check out the output:\nopenat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;err\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 dup2(3, 2) = 2 strace: Process 11593 attached [pid 11593] openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libtinfo.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;/dev/tty\u0026quot;, O_RDWR|O_NONBLOCK) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64-linux-gnu/gconv/gconv-modules.cache\u0026quot;, O_RDONLY) = 3 [pid 11593] openat(AT_FDCWD, \u0026quot;./test\u0026quot;, O_RDONLY) = 3 [pid 11593] dup2(3, 255) = 255 [pid 11593] dup2(2, 1) = 1 [pid 11593] write(1, \u0026quot;Hello,\\n\u0026quot;, 7) = 7 [pid 11593] dup2(10, 1) = 1 [pid 11593] dup2(2, 1) = 1 [pid 11593] write(1, \u0026quot;World!\\n\u0026quot;, 7) = 7 [pid 11593] dup2(10, 1) = 1 [pid 11593] +++ exited with 0 +++ --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=11593, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- dup2(10, 2) = 2 +++ exited with 0 +++  Notice this time, our file err is only opened and truncated once, before the child process starts! The child process simply copies value of file descriptor 2 to 1, which redirects STDOUT to STDERR and immediately writes output of echo to STDOUT (points to err file at this point). Since there is no additional openat() call, the err file does not get truncated between two echo calls!\nIntuitively, we can postulate that, when given a file path to redirect to, Linux will always attempt to open the file and assign a file descriptor to it, truncating the file in the process. But when redirecting with \u0026gt;\u0026amp;, Linux simply copies file descriptor, without attempting to open again (a valid file descriptor should indicate that the file is open and ready for I/O).\nTo verify this speculation, let\u0026rsquo;s try two commands and compare them:\n$ strace -f -e trace=openat,write,dup2 sh -c 'ls 1\u0026gt;\u0026amp;2' openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 dup2(2, 1) = 1 strace: Process 12024 attached [pid 12024] openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libselinux.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64-linux-gnu/libpcre2-8.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libpthread.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/proc/filesystems\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12024] openat(AT_FDCWD, \u0026quot;.\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 [pid 12024] write(1, \u0026quot;anaconda3\\tbin\\t Documents exampl\u0026quot;..., 119anaconda3\tbin\tDocuments examples.desktop learn_linux NASA_APOD output\tR\tsnap\ttoken.pickle WorkGoogleDrive ) = 119 [pid 12024] write(1, \u0026quot;android-studio\\tDesktop Download\u0026quot;..., 120android-studio\tDesktop Downloads go\tlocal\tOneDrive Pictures\trestore.spst Templates Videos Zotero ) = 120 [pid 12024] write(1, \u0026quot;Apps\\t\\tdlang\\t err\\t guix-instal\u0026quot;..., 89Apps\tdlang\terr\tguix-install.sh Music\tout\tPublic\tscripts test\tvim ) = 89 [pid 12024] +++ exited with 0 +++ --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=12024, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- dup2(10, 1) = 1 +++ exited with 0 +++  $ strace -f -e trace=openat,write,dup2 sh -c 'ls \u0026gt;/dev/stderr' openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 openat(AT_FDCWD, \u0026quot;/dev/stderr\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3 dup2(3, 1) = 1 strace: Process 12080 attached [pid 12080] openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libselinux.so.1\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libc.so.6\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/usr/lib/x86_64-linux-gnu/libpcre2-8.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libdl.so.2\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/lib/x86_64-linux-gnu/libpthread.so.0\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/proc/filesystems\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;/usr/lib/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 [pid 12080] openat(AT_FDCWD, \u0026quot;.\u0026quot;, O_RDONLY|O_NONBLOCK|O_CLOEXEC|O_DIRECTORY) = 3 [pid 12080] write(1, \u0026quot;anaconda3\\tbin\\t Documents exampl\u0026quot;..., 119anaconda3\tbin\tDocuments examples.desktop learn_linux NASA_APOD output\tR\tsnap\ttoken.pickle WorkGoogleDrive ) = 119 [pid 12080] write(1, \u0026quot;android-studio\\tDesktop Download\u0026quot;..., 120android-studio\tDesktop Downloads go\tlocal\tOneDrive Pictures\trestore.spst Templates Videos Zotero ) = 120 [pid 12080] write(1, \u0026quot;Apps\\t\\tdlang\\t err\\t guix-instal\u0026quot;..., 89Apps\tdlang\terr\tguix-install.sh Music\tout\tPublic\tscripts test\tvim ) = 89 [pid 12080] +++ exited with 0 +++ --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=12080, si_uid=1000, si_status=0, si_utime=0, si_stime=0} --- dup2(10, 1) = 1 +++ exited with 0 +++  Exactly what we predicted! We see that when redirected with \u0026gt;\u0026amp;2, the function openat(AT_FDCWD, \u0026quot;/dev/stderr\u0026quot;, O_WRONLY|O_CREAT|O_TRUNC, 0666) is not called, while when redirected with \u0026gt;/dev/stderr, it does get called.\nTo summarize\u0026hellip; You should pretty much always use \u0026gt;\u0026amp; instead of /dev/stderr or /dev/stdout when manipulating STDIO streams. Although, I stumbled upon this Bash manual page:\n","date":1578528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578528000,"objectID":"87be8ede706596b3434532ddf3a7e1dd","permalink":"/post/dev-stdout-vs-2-a-dive-into-linux-stdio/","publishdate":"2020-01-09T00:00:00Z","relpermalink":"/post/dev-stdout-vs-2-a-dive-into-linux-stdio/","section":"post","summary":"I was assisting a bioinformatics workshop for FAANG group at Plant and Animal Genome (PAG) and I ran into an interesting question with an example provided by one of the instructors on the topic of Linux output redirecting. Take a look at this simplified example script print_stderr.sh:\n#!/usr/bin/bash echo \u0026quot;first line of stderr\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;second line of stderr\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;third line of stderr\u0026quot; \u0026gt; /dev/stderr echo \u0026quot;A line of stdout\u0026quot;  Now if we redirect the stderr of this script to a file, instead of all three lines of stderr messages, we only see the last line!","tags":["Linux"],"title":"/dev/stderr vs \u003e\u00262: a dive into Linux STDIO","type":"post"},{"authors":null,"categories":["Workflow"],"content":" Why use Snakemake on HPC Snakemake is a handy workflow manager written in Python. It handles workflow based on predefined job dependencies. One of the great features of Snakemake is that it can manage a workflow on both a standalone computer, or a clustered HPC system. HPC, or \u0026ldquo;cluster\u0026rdquo; as it\u0026rsquo;s often referred to, requires additional considerations.\nOn HPC, all computing jobs should be submitted to \u0026ldquo;compute nodes\u0026rdquo; through a workload manager (for example, Slurm). Running jobs on \u0026ldquo;heaed nodes\u0026rdquo;, or log-in nodes, is a big no-no: doing so risks exhausting computing resources of head node computer (which is very limited) and slowing down everyone else on that same head node.\nFile system is another thing that requires consideration when computing on HPC. Many HPC systems use NFS (Network File System) to have storage system accessible to all computing and head nodes. But some HPC systems also have dedicated local storage for each compute node. This is to help offload I/Os from major storage drives when users are running jobs with intensive I/O. It is important that users make use of these local storage spaces, as well as clean up their temporary files.\nBelow I will show how to achieve responsible computing on HPC with a few tweaks to your Snakemake workflow.\nHow to run snakemake on HPC Snakemake natively supports managing job submission on HPC. Take a look at their documentation here.\nLet\u0026rsquo;s assume I have a Snakefile with following minimal rules:\nrule all: input: \u0026quot;RawData/Sample.fastq.gz\u0026quot; rule compressFASTQ: input: \u0026quot;RawData/Sample.fastq\u0026quot; output: \u0026quot;RawData/Sample.fastq.gz\u0026quot; shell: \u0026quot;\u0026quot;\u0026quot; gzip {input} \u0026quot;\u0026quot;\u0026quot;  To execute this workflow locally, we can simply invoke snakemake -s path/to/Snakefile (or snakemake if you\u0026rsquo;re in the same directory as Snakefile).\nNow if we want to run this on HPC on compute nodes, Snakemake allows us to do so by:\nsnakemake -s Snakefile --cluster 'sbatch -t 60 --mem=2g -c 1'  In the above command, we use --cluster to hand Snakemake a command that it can use to submit a job script. Snakemake creates a job script for each job to be run and uses this command to submit that job to HPC. That simple! Notice that here we use sbatch because it\u0026rsquo;s the command you would use to submit a shell script on HPC managed by Slurm. Replace this with qsub commands if your HPC uses SGE.\nDecorate your job submission Claim different resources based on rules Now suppose we have a slightly more complicated workflow:\nsamples = ['sample1', 'sample2', 'sample3', 'sample4'] rule all: input: expand(\u0026quot;Alignment/{sample}.bam\u0026quot;, sample = samples) rule downloadFASTQ: output: \u0026quot;RawData/{sample}_1.fastq.gz\u0026quot;, \u0026quot;RawData/{sample}_2.fastq.gz\u0026quot; shell: \u0026quot;\u0026quot;\u0026quot; ascp username@host:path/to/file/{wildcards.sample} RawData/ \u0026quot;\u0026quot;\u0026quot; rule mapFASTQ: input: f1 = \u0026quot;RawData/{sample}_1.fastq.gz\u0026quot;, f2 =\u0026quot;RawData/{sample}_2.fastq.gz\u0026quot;, ref = \u0026quot;Ref/ref.fa\u0026quot; output: temp(\u0026quot;Alignment/{sample}.sam\u0026quot;) shell: \u0026quot;\u0026quot;\u0026quot; bwa mem -R \u0026quot;@RG\\\\tID:{wildcards.sample}\\\\tSM:{wildcards.sample}\u0026quot; {input.ref} {input.f1} {input.f2} \u0026gt; {output} \u0026quot;\u0026quot;\u0026quot; rule convertSAM: input: \u0026quot;Alignment/{sample}.sam\u0026quot; output: \u0026quot;Alignment/{sample}.bam\u0026quot; shell: \u0026quot;\u0026quot;\u0026quot; samtools view -bh {input} \u0026gt; {output} \u0026quot;\u0026quot;\u0026quot;  Now in this workflow, different jobs require different computing resources: mapFASTQ probably requires more RAM than downloadFASTQ does. And more threads would probably speed up mapFASTQ but change little to downloadFASTQ. Using --cluster like shown above will cause all jobs to request same amount of resources from HPC. How can we make Snakemake request different amount of resources based on each job?\nThis can be easily achieved by two ways:\n Define resources in rule parameters:  The string following --cluster can access all variables within a rule like we could do in a shell command within a rule. For example, we can define rule mapFASTQ shown above as this instead:\nrule mapFASTQ: input: f1 = \u0026quot;RawData/{sample}_1.fastq.gz\u0026quot;, f2 =\u0026quot;RawData/{sample}_2.fastq.gz\u0026quot;, ref = \u0026quot;Ref/ref.fa\u0026quot; output: temp(\u0026quot;Alignment/{sample}.sam\u0026quot;) threads: 8 params: time = 120, mem = \u0026quot;8g\u0026quot; shell: \u0026quot;\u0026quot;\u0026quot; bwa mem -@ {threads} -R \u0026quot;@RG\\\\tID:{wildcards.sample}\\\\tSM:{wildcards.sample}\u0026quot; {input.ref} {input.f1} {input.f2} \u0026gt; {output} \u0026quot;\u0026quot;\u0026quot;  And then we can revoke the workflow by:\nsnakemake -s Snakefile --cluster 'sbatch -t {params.time} --mem={params.mem} -c {threads}'  Notice that you must define these params for all rules or you will get an error when Snakemake tries to submit a job from a rule that doesn\u0026rsquo;t have one or more of these params defined.\nAnother more convinient way involves using a separate cluster config file:\n Define resources in cluster.config:  Instead of putting parameters for cluster submission into each individual rule, you can write a separate config file that contains these parameters:\ncluster.yaml:\n__default__: partition: \u0026quot;high\u0026quot; cpus: \u0026quot;{threads}\u0026quot; time: 60 mem: \u0026quot;2g\u0026quot; mapFASTQ: mem: \u0026quot;8g\u0026quot; time: 120 convertSAM: mem: \u0026quot;4g\u0026quot;  This file defines dafault parameters Snakemake will use for job submissions for all rules, unless they are overridden by specific parameters defined below.\nNow in our rule definition, we only need to specify threads:\nrule mapFASTQ: input: f1 = \u0026quot;RawData/{sample}_1.fastq.gz\u0026quot;, f2 =\u0026quot;RawData/{sample}_2.fastq.gz\u0026quot;, ref = \u0026quot;Ref/ref.fa\u0026quot; output: temp(\u0026quot;Alignment/{sample}.sam\u0026quot;) threads: 8 shell: \u0026quot;\u0026quot;\u0026quot; bwa mem -@ {threads} -R \u0026quot;@RG\\\\tID:{wildcards.sample}\\\\tSM:{wildcards.sample}\u0026quot; {input.ref} {input.f1} {input.f2} \u0026gt; {output} \u0026quot;\u0026quot;\u0026quot;  The reason I keep threads in rule definition is because when running on a local machine Snakemake can still utilize this paramter for multi-threading\nThen we can revoke by:\nsnakemake -s Snakefile --cluster-config cluster.yaml --cluster 'sbatch -t {cluster.time} --mem={cluster.mem} -c {cluster.cpus}'  Redirect cluster output When running a job on cluster, stdout and stderr are not immediately obvious. Slurm managed systems redirect them to slurm-JOBID.txt in working directory by default. This can be hard to tracked down when you\u0026rsquo;re using Snakemake to manage more than a few dozen jobs. Luckily, with Snakemake this can be very easily changed. Let\u0026rsquo;s modify out cluster.yaml file:\n__default__: partition: \u0026quot;high\u0026quot; cpus: \u0026quot;{threads}\u0026quot; time: 60 mem: \u0026quot;2g\u0026quot; output: \u0026quot;logs_slurm/{rule}.{wildcards}.out\u0026quot;  And then we revoke Snakemake by:\nsnakemake -s Snakefile --cluster-config cluster.yaml --cluster 'sbatch -t {cluster.time} --mem={cluster.mem} -c {cluster.cpus} -o {cluster.output} -e {cluster.output}'  This will redirect stdout and stderr of every job to their corresponding log files named as by rule and wildcards (eg. downloadFASTQ.sample1.out) under logs_slurm directory. If a particular job fails, it\u0026rsquo;s super easy to find its log file!\nEmail notification when a job fails Often times it\u0026rsquo;s desirable to know when a job fails. You can have cluster notify you by email simply by modifying cluster.yaml to include cluster params for email notifications. For example:\n__default__: partition: \u0026quot;high\u0026quot; cpus: \u0026quot;{threads}\u0026quot; time: 60 mem: \u0026quot;2g\u0026quot; output: \u0026quot;logs_slurm/{rule}.{wildcards}.out\u0026quot; email: \u0026quot;user@host.com\u0026quot; email_type: \u0026quot;FAIL\u0026quot;  And then revoke Snakemake with:\nsnakemake -s Snakefile --cluster-config cluster.yaml --cluster 'sbatch -t {cluster.time} --mem={cluster.mem} -c {cluster.cpus} -o {cluster.output} -e {cluster.output} --mail-type {cluster.email_type} --mail-user {cluster.email}'  Put Snakemake command in a \u0026ldquo;scheduler\u0026rdquo; script Evidently, as our cluster config parameters grow, the command to revoke Snakemake also grows and it becomes tedious to type every time. To avoid this, you can simply put this command in a sbtach script like so:\nscheduler.sh\n#! /bin/bash -login #SBATCH -J scheduler #SBATCH -t 7-00:00:00 #SBATCH -N 1 #SBATCH -n 1 #SBATCH -c 1 #SBATCH -p high #SBATCH --mem=2gb #SBATCH --mail-type=ALL #SBATCH --mail-user=user@host.com cd /path/to/snakemake/piepeline mkdir -p logs_slurm snakemake -s Snakefile --cluster-config cluster.yaml --cluster 'sbatch -t {cluster.time} --mem={cluster.mem} -c {cluster.cpus} -o {cluster.output} -e {cluster.output} --mail-type {cluster.email_type} --mail-user {cluster.email}'  And then submit this script to cluster using sbatch scheduler.sh. Then all you need to do is sit back and wait for it to finish (or error out).\nBest practice on cluster Use local temp directory Because of the unique structures of HPC systems, many utilize local storage to speed up I/O intensive jobs. Therefore, whenever you have jobs that read/write (especially in small chunks) intensively to hard disks, you should try to have them I/O on these local storage and move your files once the job is finished.\nSuppose we have /scratch/ as local space for each node on our HPC system. Reading from and writing to /scratch/ will be significantly faster than any other directories. But you can\u0026rsquo;t access this space from any other nodes except this one, including your head node. Therefore you should always move your desired files afterwards. And these storage spaces are usually very limited (~1Tb), you should also always clean up after your job so others can make use of it as well.\nTo utilize this space, in your bash script, define a cleanup function and call it upon exit:\nexport MYTMP=/scratch/$USER/$SLURM_JOBID cleanup() {{ rm -rf $MYTMP; }} trap cleanup EXIT mkdir -p $MYTMP ...  In the above code, we defined our temp directory inside /scratch/ using $USER and $SLURM_JOBID. This allows anyone, including yourself, to be able to know immediately which directory belongs to you and whether or not it is currently in use (depending on the state of associated job).\nYou can add this code to your Snakefile whenever you use shell directive. trap keyword will ensure that cleanup() function is always called to remove all temp files associated with a job whenever said job exits, either successfully or due to a failure.\nRun interactive jobs on compute nodes When analyzing data, we often need to run commands interactively, either testing out commands before deploying at a large scale or run some preliminary data exploration and visualization through Rstudio or Jupyter Notebook. When working on HPC systems, a rule of thumb is to not run any jobs other than simple commands like cp, mv, nano, etc on head nodes because head nodes usually have very limited resources and are not suitable for resource-intensive computing. To run commands interactively, you need to start an interactive job:\nsrun -t 240 --mem=10g --pty bash  This will request 10g RAM and 1 thread for 240 minutes (4 hours) to start an interactive bash shell. You can also use --pty R and --pty python to start R and Python shells respectively.\nRun RStudio interactively on HPC I\u0026rsquo;m a big fan of RStudio for its user friendly interface and versatile functionality and I use R notebooks with RStudio for most of my data analysis work and I often run out of RAM on my PC for large data-set. Is it possible to move my RStudio workflow to HPC as well? Yes!\nFirst let\u0026rsquo;s install RStudio on HPC. I use conda but many other methods will do as well.\nconda create -n rstudio rstudio  And then fresh log in to HPC through ssh with X11 forwarding enabled:\nssh -X username@host  You can test X11 forwarding with your connection with xclock. If it\u0026rsquo;s working, you should see an analog clock app pop up on your local machine. Now we can start RStudio. First request an interactive session:\nsrun -t 240 --mem=10g --pty bash  And start RStudio:\nconda activate rstudio rstudio  Now you should be able to use it on your local machine!\n","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"2f5d4b30ad69f95963f4ab3d9f050cf7","permalink":"/post/how-to-run-snakemake-pipeline-on-hpc/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/post/how-to-run-snakemake-pipeline-on-hpc/","section":"post","summary":"Why use Snakemake on HPC Snakemake is a handy workflow manager written in Python. It handles workflow based on predefined job dependencies. One of the great features of Snakemake is that it can manage a workflow on both a standalone computer, or a clustered HPC system. HPC, or \u0026ldquo;cluster\u0026rdquo; as it\u0026rsquo;s often referred to, requires additional considerations.\nOn HPC, all computing jobs should be submitted to \u0026ldquo;compute nodes\u0026rdquo; through a workload manager (for example, Slurm).","tags":["bioinformatics","Workflow"],"title":"How to run snakemake pipeline on HPC","type":"post"},{"authors":null,"categories":["Genetics"],"content":" At the moment of writing this post, I\u0026rsquo;m sitting on the plane flying to Cornell. There I will learn a beautiful technique called Chromatin Run-On and Sequencing, or ChRO-seq.\nThirteen hours of travel is getting to the point of breaking my soul \u0026ndash; I have finished most of work I can comfortablely do with one screen on a samll laptop. So I decided to write down what I know so far about ChRO-seq in preparation for my upcoming training tomorrow. I will keep updating this post as I learn more about this technique.\nWhat is ChROseq ChRO-seq is a variant of Run-On sequencing developed by Danko lab at Cornell (Chu et. al., 2018). In essence, it profiles nascent RNA by capturing RNA polII and DNA interaction. Below is a diagram from the authors depicting the major steps of a ChRO-seq experiment:\nAs shown above, insoluble chromatin is first isolated from tissues or cell culture. Isolated chromatin then gets re-suspended through sonication. Run-on reaction is done by incubating RNA polII and DNA complex with biotinylated rNTP. A complementary biotinylated NTP gets added on to the attached nascent RNA and prevents any further elongation.\nOnce run-on reaction is completed, 3 rounds of streptavidin bead purification are performed to purify extended nascent RNA, add 3\u0026rsquo; adapter, remove 5\u0026rsquo;cap and add 5\u0026rsquo; adapter. After adapters are added to both ends, a cDNA library is constructed from the nascent RNAs and a single end illumina sequencing is performed from 3\u0026rsquo;end.\nWhat does ChRO-seq tell me Like any run-on sequencing techique, ChRO-seq precisely captures nascent RNA when compared to general RNAseq. This means that:\n ChRO-seq gives a more accurate quantification of DNA transcription, as opposed to RNAseq, which describes a result of DNA transcription and RNA degradation. The fact that ChROseq is not noised by RNA degradation also means that it is able to capture many non-coding RNAs that have a short life span in cells and thus difficult for RNAseq to capture. ChRO-seq provides a snap shot of transcription activity. As revealed by other run-on sequencing, it can identify RNA polII pausing sites. Like other run-on sequencing, ChRO-seq can also be used to identify different types of transcriptions, as observed by Core et. al., 2008.  What does ChRO-seq not tell me Because ChRO-seq captures nascent RNA, naturally, it does not contain any post-transcriptional level information like splicing, polyadenylation, and other RNA or protein modifications.\nHow does ChRO-seq compare to other RNA-related techniques?     RNAseq ChRO-seq PRO-seq GRO-seq     RNA captured mature RNA Nascent RNA Nascent RNA Nascent RNA   Quantify transcription Yes(with degradation) Yes Yes Yes   Single base resolution Yes Yes Yes No   Material type RNA Insoluble chromatin Nuclei Nuclei    ","date":1565481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565481600,"objectID":"a5bb3d306aa01df2f56b0a1dc928e0e7","permalink":"/post/what-is-chroseq-and-how-does-it-work/","publishdate":"2019-08-11T00:00:00Z","relpermalink":"/post/what-is-chroseq-and-how-does-it-work/","section":"post","summary":"At the moment of writing this post, I\u0026rsquo;m sitting on the plane flying to Cornell. There I will learn a beautiful technique called Chromatin Run-On and Sequencing, or ChRO-seq.\nThirteen hours of travel is getting to the point of breaking my soul \u0026ndash; I have finished most of work I can comfortablely do with one screen on a samll laptop. So I decided to write down what I know so far about ChRO-seq in preparation for my upcoming training tomorrow.","tags":["Sequencing","bioinformatics"],"title":"What is ChROseq and how does it work","type":"post"},{"authors":null,"categories":null,"content":" ezProtocol is a program designed for Opentrons OT2 robot. It handles writing detailed script for you so you can spend your time on things that matter. It can be found here.\nezProtocol can read protocol file in a format designed to be easy but versatile.\nSee here on how to make a protocol file.\nRequirements ezProtocol relies on below packages: - PyYAML - Pandas - python-frontmatter - regex - opentrons\nInstallation (pip deploy pending) Clone this repo:\ngit clone https://github.com/SichongP/ezProtocol  Change directory:\ncd ezProtocol  Install package:\npip install .  Or, use install.sh script:\nbash install.sh  Test installation:\nezprotocol -h  Getting started Once you have protocol and deck layout file ready (see here for more detail on protocol and layout format), on commandline, type:\nezprotocol -p protocol.txt -d deck_layout.csv -o ot2_script.py  ","date":1565385074,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565385074,"objectID":"2687d57d5ff76e4709ce4d5d06781ad8","permalink":"/project/ezprotocol/","publishdate":"2019-08-09T14:11:14-07:00","relpermalink":"/project/ezprotocol/","section":"project","summary":"ezProtocol is a program designed for Opentrons OT2 robot. It handles writing detailed script for you so you can spend your time on things that matter. It can be found here.\nezProtocol can read protocol file in a format designed to be easy but versatile.\nSee here on how to make a protocol file.\nRequirements ezProtocol relies on below packages: - PyYAML - Pandas - python-frontmatter - regex - opentrons","tags":[],"title":"EzProtocol","type":"project"},{"authors":null,"categories":null,"content":" Introduction If you have ever written a scientific paper in MS Word (or any other word processing software) then you\u0026rsquo;re probably experienced frustrations like me: Word constantly crashing when loaded with too many high-resolution figures, citations getting messed up when working with others that use different citation managers, or hard to keep track of hundreds of versions of the draft, etc.\nOf course, Latex is always an excellent solution to all those problems. It separates content and formatting, allowing you to focus on content and leave the rest to those with expertise at it. A Tex file is also a pure text format so it\u0026rsquo;s easy to manage with a version control tool like git. However, Latex does have a rather steep learning curve. And a Tex file is not the most intuitive format to look at. For example, look at the below Tex format: Another problem with writing in Latex is, while it\u0026rsquo;s not easy to learn Latex, it\u0026rsquo;s even harder to get all your collaborators on board, which is usually the case with scientific papers.\nThat led me to think: markdown is a fantastic format that is both intuitive and powerful. Why not write in markdown? After a short search, I\u0026rsquo;m happy to report that this is indeed possible!\nWhat does writing a scientific paper require? Content aside, to write a scientific paper efficiently, one needs at least the following elements:\n Easy way to insert and manage citations. Easy way to insert and manage high resolution figures Easy way to format tables Can be converted to other formats (pdf, html, etc) Allows collaborative writing  Atom is an excellent text editor that with packages can do almost all of the above!\nRequired software and packages  Atom\n markdown-preview-enhanced zotero-citations  Pandoc\n A Tex distribution for your OS (I used Tex Live for my linux system)\n Zotero citation manager\n Bbibtex Add-on   Once you have all required packages installed, it\u0026rsquo;s time to start writing!\nWrite a paper Writing a paper in markdown is as easy as it sounds: use # for titles and subtitles, use | to format a table, among other things.\nWrite math equations You can use Latex syntax to write a math equation, like so:\n$f_A^2+2f_Af_B+f_B^2=1$  $f_A^2+2f_Af_B+f_B^2=1$\nImport figures: You can insert a figure in typical markdown way:\n![alt text](figure.png)  This would import the figure at current location. You can even further configure the figure like so:\n![alt tex](figure.png){#id .class width=300px height=200px}  Note: Because markdown preview enhanced by default uses a different render engine to render the preview, you will not see change to figure size in the preview but it will be rendered when converting to another output. You can also change its default markdown render engine to pandoc to see size change in preview.\nInsert citations: In order to insert a citation, you need to have zotero open. With zotero open, you can call up zotero citation picker by either pressing ctrl + alt + P or pressing ctrl + shift + P to call up the command palate and search for \u0026ldquo;zotero citations: pick\u0026rdquo;. In the citation picker window like below: type in the reference you wish to insert and hit enter. A string of citation key should appear in your document:\n@bryoisEvaluationChromatinAccessibility2018  Surround this reference key with [] and you\u0026rsquo;re good to go! Once you export this document to a pdf, word, or html format, the reference will be automatically appended to the end of your document and the ref key will be replaced with corresponding references, like so: To add multiple citations, use ; to separate ref keys, like so:\n[@bintuSuperresolutionChromatinTracing2018; @bryoisEvaluationChromatinAccessibility2018]  Lastly, in order for pandoc to render citations correctly, you must provide a bib file that contains all your citations and their corresponding ref keys.\nTo do this, head on to zotero and click File -\u0026gt; Export library, in the format section, choose Better Bibtex. If you want the exported library up to date with your zotero library, check Keep updated. Once you have the exported library, add yaml frontmatter to the beginning of your document, like so:\n--- bibliography: My Library.bib ---  Export to other formats With pandoc, you can easily convert the markdown format to other text formats like pdf, word, and tex. To do this, first add output: word_document to your document\u0026rsquo;s frontmatter, like so:\n--- bibliography: My Library.bib output: word_document ---  and then you can right clock on the preview panel on the right to export the document through pandoc: This will output your document to a Word document. To output to PDF, simple change the value of output in frontmatter to pdf_document.\nConclusion Markdown is an incredibly intuitive text format and yet still versatile and powerful enough for productive writing. It has recently surged to new favorite of many among developers, scientists, and journalists. It will certainly keep raising in popularity due to its friendliness and expandability.\nWriting a scientific document in markdown has freed me of many frustrations caused by Word and with incredible pandoc, I never have to wrestle with any of my collaborators over which text editor we are to use. It has been a life saver!\n","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"940db2ad8fa86b0098f2a093182d2c4d","permalink":"/post/2019-08-07-write-scientific-paper-in-markdown/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/2019-08-07-write-scientific-paper-in-markdown/","section":"post","summary":"Introduction If you have ever written a scientific paper in MS Word (or any other word processing software) then you\u0026rsquo;re probably experienced frustrations like me: Word constantly crashing when loaded with too many high-resolution figures, citations getting messed up when working with others that use different citation managers, or hard to keep track of hundreds of versions of the draft, etc.\nOf course, Latex is always an excellent solution to all those problems.","tags":null,"title":"Write scientific paper in markdown","type":"post"},{"authors":null,"categories":["Genetics"],"content":" Table of content  Table of content What Are Haplotype and Linkage Disequilibrium (LD) Quantify LD  Linkage Disequilibrium Coefficient ($D$): Standardized Linkage Disequilibrium Coefficient ($D\u0026rsquo;$) Correlation coefficient ($\\gamma^2$)  Now why do we care about LD and haplotype?  LD is an indication of deviation from Hardy-Weinberg equilibrium.   This is the first post of my QE prep series. I\u0026rsquo;ll be taking notes on my study for general knowledge section of my QE. I\u0026rsquo;m gonna start this off with my favorite topic in our GGG series classes: population genetics\nWhat Are Haplotype and Linkage Disequilibrium (LD) To properly understand linkage disequilibrium, I want to go back to the \u0026ldquo;Dark Age\u0026rdquo; of genetics, when Gregor Mendel brilliantly discovered (Mendelian) laws of inheritance without any knowledge of the molecular basis.\n The First Mendelian Law (Law of Segregation) states that each organism has two alleles for each trait and one of the two alleles is randomly passed on to an offspring during sexual reproduction.\n For example, an individual with an AB genotype is eqaully likely to pass an A allele or a B allele to its offspring (there are many assumption being made in this statement but I\u0026rsquo;ll roll with it for now).  The Second Mendelian Law (Law of Independent Assortment) states that alleles for different traits are passed on independantly from each other during sexual reproduction.\n For example, if allelea A/a controls for trait 1 and alleles B/b controls for trait 2 (assuming they are not at the same locus), then whether an individual passes on allele A or a is independent of wheter it passes on B or b. The Independent Assortment implicts the product rule:\n Suppose an individual with AaBb genotype (Aa at locus 1 and Bb at locus 2) mates with another individual with same AaBb genotype, the probability of their offspring being aabb is 1/16\n In case you\u0026rsquo;re wondering how to get this number, here is my favorite way to calculate:\n   In the above diagram, first step involves the Law of Segregation, by which I calculated the probability of each gamete containing each allele separately. The second step is where the Law of Independent Assortment comes into play: The probability of each combination of two alleles is the product of the probabilities of each allele (in one gamete). And the last step is just random combination of two gametes (for a diploid organism).\nHowever, just as every law has an exception, it was discovered that many of traits in various species didn\u0026rsquo;t really follow the Law of Independent Assortment. The following example shows a case of \u0026ldquo;dependent assortment\u0026rdquo;:\n Sweet peas have purple (P) or red (p) flowers and long (L) or short (l) pollen grains. Peas from two pure lines (PPLL and ppll) were crossed. The offspring (F1) are all purple flower and long grain (PpLl). They are then self crossed (PpLl X PpLl).\nAssuming indenpendent assortment, we expect to observe all 4 phenotypes in F2 with the following frequencies:\n    Phenotype Frequency     Purple (P), Long (L) 9\u0026frasl;16   Purple (P_), Short (ll) 3\u0026frasl;16   Red (pp), Long (L_) 3\u0026frasl;16   Red (pp), Short (ll) 1\u0026frasl;16     However, large deviations from expected phenotype frequency were observed:\n    Phenotype Exp. Count Obs. Count     Purple, Long (P_L_) 216 284   Purple, Short (P_ll) 72 21   Red, Long (ppL_) 72 21   Red, Short (ppll) 24 55    Data from Bateson et al.\nWe know that in the parent line (PPLL x ppll) P and L as well as p and l are always together because they are both homozygotes. But as the Mendel\u0026rsquo;s second law predicts, they should\u0026rsquo;ve independently assorted from F1 to F2. We, however, see that instead of random combination, P still appear more often together with L and so does p and l. It\u0026rsquo;s as if P and L were somehow linked. This phenomenon is termed Linkage. We now know that this is because the two loci are closeby on the same chromsome and that during meiosis, DNA on one chromsome are largely segregated together (except some recombination).\nWhen two loci are linked, product rule no longer applies. This causes a non-random association of alleles at different loci in a given population. This is termed Linkage Disequilibrium or LD. (Technically LD does not always mean physical linkage. More on this later.)\nWhen we talk about linkage, knowing genotype of an individual alone is no longer enough: we wish to know which alleles are \u0026ldquo;linked\u0026rdquo; or belong to the same chromsome. In genetics, we call alleles that are on the same chromsome and likely to be inherited together a haplotype. For example, an individual with a PpLl genotype may have these different haplotypes:\n PL and pl Pl and pL  Quantify LD There are several different ways to quntify the extent of linkage between any two loci:\n Coefficient of Linkage Disequilibrium (D) Normalized coefficient of Linkage Disequilibrium (D\u0026rsquo;) Correlation coefficient ($r^2$) and $\\chi^2$ test  I\u0026rsquo;ll briefly go over them below.\nLinkage Disequilibrium Coefficient ($D$): As mentioned before, if two loci are independent, their frequencies follow product rule:\nFor two independent loci A and B, each with two alleles ($A_1$, $A_2$ and $B_1$, $B_2$, respectively), we have: $$f_{A_1B_1} = f_{A_1}f_{B_1}$$ Now if A and B are not independent, this equation no longer stands true. In that case we have: $$f_{A_1B_1} = f_{A_1}f_{B_1} + D$$ Here we have D as a measurement of the extent of linkage disequilibrium between $A_1$ and $B_1$. We can calculate the same for all 4 haplotypes:\n    A $A_1$ $A_2$     B frequency $f_{A_1}$ $f_{A_2}$   $B_1$ $f_{B_1}$ $f_{A_1}f_{B_1} + D_{A_1B_1}$ $f_{A_2}f_{B_1} + D_{A_2B_1}$   $B_2$ $f_{B_2}$ $f_{A_1}f_{B_2} + D_{A_1B_2}$ $f_{A_2}f_{B_2} + D_{A_2B_2}$    Solve above equations and we get:\n $D_{A_1B_1} = f_{A_1B_1} - f_{A_1}f_{B_1}$\n$D_{A_1B_2} = f_{A_1B_2} - f_{A_1}f_{B_2}$\n$D_{A_2B_1} = f_{A_2B_1} - f_{A_2}f_{B_1}$\n$D_{A_2B_2} = f_{A_2B_2} - f_{A_2}f_{B_2}$\n For a given population, we have\n $f_{A_1} + f_{A_2} = 1$\n$f_{B_1} + f_{B_2} = 1$\n$f_{A_1B_1} + f_{A_1B_2} = f_{A_1}$\n$f_{A_1B_1} + f_{A_2B_1} = f_{B_1}$\n$f_{A_2B_1} + f_{A_2B_2} = f_{A_2} = 1 - f_{A_1}$\n$f_{A_1B_2} + f_{A_2B_2} = f_{B_2} = 1 - f_{B_1}$\n We can then further derive above equations:\n $D_{A_2B_2} \\\\\\ = f_{A_2B_2} - (1 - f_{A_1})(1 - f_{B_1}) \\\\\\\\ = 1 - f_{B_1} - f_{A_1B_2} - (1 - f_{A_1})(1 - f_{B_1}) \\\\\\ = 1 - f_{B_1} - f_{A_1} - f_{A_1B_1} - (1 - f_{A_1})(1 - f_{B_1}) \\\\\\ = 1 - f_{B_1} - f_{A_1} + f_{A_1B_1} - 1 + f_{B_1} + f_{A_1} - f_{A_1}f_{B_1} \\\\\\ = f_{A_1B_1} - f_{A_1}f_{B_1} = D_{A_1B_1}$\n Similarly we can prove that $D_{A_1B_1} = - D_{A_1B_2} = - D_{A_2B_1} = D_{A_2B_2} = D$ (note that this is only true for diallelic loci. For loci with more than two alleles, we need to calculate D for each allele pair separately.)\nThis makes sense because linkage disequilibrium should be a measurement of two loci not any two specific alleles in those loci. Therefore the D for any haplotype between the two given loci should be the same (or at least the absolute value of it). We then have:\n $D = D_{A_1B_1} = f_{A_1B_1} - f_{A_1}f_{B_1}$\n$D = -D_{A_1B_2} = -f_{A_1B_2} + f_{A_1}f_{B_2}$\n$D = -D_{A_2B_1} = -f_{A_2B_1} + f_{A_2}f_{B_1}$\n$D = D_{A_2B_2} = f_{A_2B_2} - f_{A_2}f_{B_2}$\n Next we can prove that $D = f_{A_1B_1}f_{A_2B_2} - f_{A_1B_2}f_{A_2B_1}$:\n $f_{A_1B_1}f_{A_2B_2} \\\\\\ = (f_{A_1}f_{B_1} + D)(f_{A_2}f_{B_2} + D) \\\\\\ = f_{A_1}f_{A_2}f_{B_1}f_{B_2} + D(f_{A_1}f_{B_1} + f_{A_2}f_{B_2}) + D^2$\n$f_{A_1B_2}f_{A_2B_1} \\\\\\ = (f_{A_1}f_{B_2} - D)(f_{A_2}f_{B_1} - D) \\\\\\ = f_{A_1}f_{A_2}f_{B_1}f_{B_2} - D(f_{A_1}f_{B_2} + f_{A_2}f_{B_1}) + D^2$\n Taske substraction:\n $f_{A_1B_1}f_{A_2B_2} - f_{A_1B_2}f_{A_2B_1} \\\\\\ = f_{A_1}f_{A_2}f_{B_1}f_{B_2} + D(f_{A_1}f_{B_1} + f_{A_2}f_{B_2}) + D^2 - (f_{A_1}f_{A_2}f_{B_1}f_{B_2} - D(f_{A_1}f_{B_2} + f_{A_2}f_{B_1}) + D^2) \\\\\\ = D(f_{A_1}f_{B_1} + f_{A_2}f_{B_2} + f_{A_1}f_{B_2} + f_{A_2}f_{B_1}) = D * 1 = D$\n Now we have a measure of linkage disequilibrium. By definition, we know that $D=0$ indicates independent loci (no linkage). What should the maxium value of D be?\n Say we have a population with following genotype frequency: $f_{A_1} = f_{A_2} = 0.5 \\\\\\ f_{B_1} = f_{B_2} = 0.5$\nSuppose there is complete linkage between $A_1$ and $B_1$, meaning we have following haplotype frequencies: $f_{A_1B_1} = f_{A_2B_2} = 0.5 \\\\\\ f_{A_1B_2} = f_{A_2B_1} = 0$\nWe can then calculate $D = f_{A_1B_1}f_{A_2B_2} - f_{A_1B_2}f_{A_2B_1} = 0.5 * 0.5 - 0 = 0.25$\n When two loci are in complete linkage, we have $D=0.25$. For any two loci, we always have $D \\in [-0.25,0.25]$.\nThis scale is not very intuitive but we can work with it. Now next question is, does equal $D$ mean equal linkage disequilibrium?\n Consider the following two populations:\n Pop 1 has haplotype frequencies as follow: $$f_{A_1B_1} = f_{A_2B_2} = 0.34 \\\\\\ f_{A_1B_2} = f_{A_2B_1} = 0.16$$ We can calculate $D = 0.34^2 - 0.16^2 = 0.09$ Pop 2 has haplotype frequencies as follow: $$f_{A_1B_1} = 0.9 \\\\\\ f_{A_2B_2} = 0.1 \\\\\\ f_{A_1B_2} = f_{A_2B_1} = 0$$ We can calculate $D = 0.9 * 0.1 - 0 = 0.09$   Even though $D$ at these loci for both populations is exactly the same, we can clearly see that pop 2 has comlete linkage between these two loci while pop 1 does not.\nThis example shows a major problem of $D$ for measuring linkage disequilibrium: It\u0026rsquo;s ignorant of allele frequencies in a given population.\nStandardized Linkage Disequilibrium Coefficient ($D\u0026rsquo;$) To address this problem, we can standardize $D$ against allele frequency: $$D\u0026rsquo; = \\frac{D}{D_{max}}$$\n$D_{max}$ is the maxium possible $D$ in a population with same allele frequencies (but different haplotype frequencies). We have $$D_{max} = f_{maxA_1B_1} - f_{A_1}f_{B_1} \\\\\\ = min(f_{A_1},f_{B_1}) - f_{A_1}f_{B_1}$$ We can also prove that for every haplotype between two given loci, we can get the same $D\u0026rsquo;$.\nFor example, in the above two populations:\n  Pop 1: $$f_{A_1} = f_{A_2} = f_{B_1} = f_{B_2} = 0.5 \\\\\\ D_{max} = 0.5 - 0.5 * 0.5 = 0.25 \\\\\\ D\u0026rsquo; = \\frac{D}{D_{max}} = \\frac{0.09}{0.25} = \\frac{9}{25} $$ Pop 2: $$f_{A_1} = f_{B_1} = 0.9 \\\\\\ f_{A_2} = f_{B_2} = 0.1 \\\\\\ D_{max} = 0.9 - 0.9 * 0.9 = 0.09 \\\\\\ D\u0026rsquo; = \\frac{D}{D_{max}} = \\frac{0.09}{0.09} = 1$$   Now we can see that $D\u0026rsquo;$ for population 2 is much higher than that of population 1, indicating a much higher linkage disequilibrium between these loci in pop 2. By definition, we know $D\u0026rsquo; \\in [0,1]$\nHowever, $D\u0026rsquo;$ also has its own problem. Consider the following scenario:\n Suppose we genotyped 100 individuals in a population, and count occurrance of each haplotype between loci A and B as follow:\n     Count Total     $A_1B_1$ 50 100   $A_1B_2$ 49 100   $A_2B_1$ 0 100   $A_2B_2$ 1 100     We can get haplotype and allele frequencies: $$f_{A_1B_1} = 0.5, f_{A_1B_2} = 0.49 \\\\\\ f_{A_2B_1} = 0, f_{A_2B_2} = 0.01 \\\\\\ f_{A_1} = 0.99, f_{A_2} = 0.01 \\\\\\ f_{B_1} = 0.5, f_{B_2} = 0.5$$ And calculate $D\u0026rsquo;$: $$D\u0026rsquo; = \\frac{f_{A_1B_1} * f_{A_2B_2} - f_{A_1B_2} * f_{A_2B_1}}{min(f_{A_1}, f_{B_1}) - f_{A_1}f_{B_1}} = \\frac{0.5 * 0.01 - 0.49 * 0}{0.5 - 0.5 * 0.99} = 1$$\n In this case, $D\u0026rsquo;$ indicates strong linkage between the two loci. However, looking at data, we can\u0026rsquo;t confidently make any conclusions regarding the linkage between the two loci. The reason is that in all samples but one, they have either $A_1B_1$ or $A_1B_2$ haplotype. The only one that has an $A_2$ allele can simply be a result of a spontaneous mutaion rather than inheriting from a parent We do not have enough information to say whether or not A locus is linked with B locus because $A_2$ is mostly missing from data.\nIn other words, $D\u0026rsquo;$ does not tell us how significant a linkage is or how confident we are in decalring a linkage disequilibrium. For this purpose, we turn to Pearson\u0026rsquo;s correlation coefficient.\nCorrelation coefficient ($\\gamma^2$) Consider the population above:\n    Count Total     $A_1B_1$ 50 100   $A_1B_2$ 49 100   $A_2B_1$ 0 100   $A_2B_2$ 1 100    We calculated that $D=0.005$, now instead of standardizing it like what we did with $D\u0026rsquo;$, we try a different approach: $$\\gamma^2 = \\frac{D^2}{f_{A_1}f_{A_2}f_{B_1}f_{B_2}}$$\nWe have $\\gamma^2 = \\frac{0.005^2}{0.99 * 0.01 * 0.5 * 0.5} = 0.01$\nNow this seems to align well with our assessment of the data! As a matter of fact, this is actually the correlation coefficient of the two variables in this population!\nLet\u0026rsquo;s arbitratrily assign 1 to an $A_1$ allele and -1 to an $A_2$ allele. And 1 to $B_1$, -1 to $B_2$. Now we can plot the data and calculate $\\gamma$:\n Correlation plot   In the above plot we have $R=0.1$ this is exactly what we calculated above ($\\gamma^2 = 0.01$)! Now that we have a correlation coefficient, we can use a $\\chi^2$ test on our dataset:\n$$\\chi_S^2=\\gamma^2*N=0.01*100=1\\\\\\ P(\\chi^2\u0026gt;\\chi_S^2, df=1) = 0.317 $$\nThe p-value is also shown in the above plot. Clearly, we don\u0026rsquo;t see a significant correlation (i.e linkage) between the two loci.\nThis is the beauty of $\\gamma^2$: it not only tells us the strength of a linkage ($\\gamma^2$) but only indicates confidence in such linkage given data ($\\chi^2$)!\nNow remember, the absence of significance is NOT evidence of insignificance! Look at the above figure and we notice that the confidence interval of the plot is very large towards $x=-1$. This is because we have only a single data point at $x=-1$. This could easily be a sampling error or like mentioned above, a spontaneous mutation. It is possible, that there are more individuals with $A_2$ allele but we just didn\u0026rsquo;t include them in our samples for unknown reasons. In this case, since we can\u0026rsquo;t estimate frequencies of $A_2$ in haplotype $A_2B_1$ or $A_2B_2$, we can\u0026rsquo;t conclude with any confidence whether or not there is linkage between the two loci.\nNow why do we care about LD and haplotype? LD is an indication of deviation from Hardy-Weinberg equilibrium. ","date":1553126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553126400,"objectID":"e526bcca6bd9373e71592039ef7c2786","permalink":"/post/haplotype-ld-and-association-tests/","publishdate":"2019-03-21T00:00:00Z","relpermalink":"/post/haplotype-ld-and-association-tests/","section":"post","summary":"Table of content  Table of content What Are Haplotype and Linkage Disequilibrium (LD) Quantify LD  Linkage Disequilibrium Coefficient ($D$): Standardized Linkage Disequilibrium Coefficient ($D\u0026rsquo;$) Correlation coefficient ($\\gamma^2$)  Now why do we care about LD and haplotype?  LD is an indication of deviation from Hardy-Weinberg equilibrium.   This is the first post of my QE prep series. I\u0026rsquo;ll be taking notes on my study for general knowledge section of my QE.","tags":["Population Genetics","QE","Statistics"],"title":"Haplotype, LD, and Association Tests","type":"post"},{"authors":null,"categories":["Python"],"content":"I want to split sequences in a fasta file at Ns.\nHere is what an example file looks like:\n\u0026gt;1_name ACGTTGCGGCATTCGATCGACGATCGATGCAAACGGTCACGGACTGACTGT ACACACGTAGCAGCATCAGCATNNNNNNNNNNNNNNNNNNNNGTTGGACGG NNNNNNNNNNNNGGTGACACACGAGATATATFAGATCAACGTAAGGGATGA NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN AGTCGCTAGCATGCATGGCATATACGCGATCGATTCGATAGCTAGCGNNNN \u0026gt;2_name ACGTTGCGGCATTCGATCGACGATCGATGCAAACGGTCACGGACTGACTGT ACACACGTAGCAGCATCAGCATATTCGATGGCATCGATACCGGTTGGACGG NNNNNNNNNNNNGGTGACACACGAGATATATFAGATCAACGTAAGGGATGA NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN AGTCGCTAGCATGCATGGCATATACGCGATCGATTCGATAGCTAGCGNNNN  There are two common formats for FASTA files:\n- Single line FASTA\nEach record consists of two line: a name line (starts with \u0026ldquo;\u0026gt;\u0026rdquo;) and a sequence line. - Multiline FASTA\nEach records consists of multiple lines, First line is a name line (starts with \u0026ldquo;\u0026gt;\u0026rdquo;), followed by multiple lines of sequences.\nHere I assume I\u0026rsquo;m dealing with multiline FASTA because if a script can work with multiline fasta, it\u0026rsquo;s generally easy to make it work with single line files.\nHere is how I approach it:\nimport sys with open('test','r') as f: seq = [] for line in f: if line.startswith(\u0026quot;\u0026gt;\u0026quot;): if seq: #seq not empty, process it trim = '\\n'.join(''.join(seq).replace(\u0026quot;N\u0026quot;,\u0026quot; \u0026quot;).split()) print(trim) seq = [] print(line.strip()) else: #Read lines into a single seq seq.append(line.strip()) if seq: #seq not empty, process it trim = '\\n'.join(''.join(seq).replace(\u0026quot;N\u0026quot;,\u0026quot; \u0026quot;).split()) print(trim) seq = []  \u0026gt;1_name ACGTTGCGGCATTCGATCGACGATCGATGCAAACGGTCACGGACTGACTGTACACACGTAGCAGCATCAGCAT GTTGGACGG GGTGACACACGAGATATATFAGATCAACGTAAGGGATGA AGTCGCTAGCATGCATGGCATATACGCGATCGATTCGATAGCTAGCG \u0026gt;2_name ACGTTGCGGCATTCGATCGACGATCGATGCAAACGGTCACGGACTGACTGTACACACGTAGCAGCATCAGCATATTCGATGGCATCGATACCGGTTGGACGG GGTGACACACGAGATATATFAGATCAACGTAAGGGATGA AGTCGCTAGCATGCATGGCATATACGCGATCGATTCGATAGCTAGCG  ","date":1553040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553040000,"objectID":"c77329396a33f989b5c5b6bcf4f1c6a8","permalink":"/post/split-a-fasta-record-by-ns/","publishdate":"2019-03-20T00:00:00Z","relpermalink":"/post/split-a-fasta-record-by-ns/","section":"post","summary":"I want to split sequences in a fasta file at Ns.\nHere is what an example file looks like:\n\u0026gt;1_name ACGTTGCGGCATTCGATCGACGATCGATGCAAACGGTCACGGACTGACTGT ACACACGTAGCAGCATCAGCATNNNNNNNNNNNNNNNNNNNNGTTGGACGG NNNNNNNNNNNNGGTGACACACGAGATATATFAGATCAACGTAAGGGATGA NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN AGTCGCTAGCATGCATGGCATATACGCGATCGATTCGATAGCTAGCGNNNN \u0026gt;2_name ACGTTGCGGCATTCGATCGACGATCGATGCAAACGGTCACGGACTGACTGT ACACACGTAGCAGCATCAGCATATTCGATGGCATCGATACCGGTTGGACGG NNNNNNNNNNNNGGTGACACACGAGATATATFAGATCAACGTAAGGGATGA NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN AGTCGCTAGCATGCATGGCATATACGCGATCGATTCGATAGCTAGCGNNNN  There are two common formats for FASTA files:\n- Single line FASTA\nEach record consists of two line: a name line (starts with \u0026ldquo;\u0026gt;\u0026rdquo;) and a sequence line. - Multiline FASTA\nEach records consists of multiple lines, First line is a name line (starts with \u0026ldquo;\u0026gt;\u0026rdquo;), followed by multiple lines of sequences.","tags":["Python","bioinformatics","FASTA"],"title":"Split a FASTA record by Ns","type":"post"},{"authors":null,"categories":["R"],"content":" Recently Someone asked a question on reddit: Adding a row and a column that are means for a set matrix\nThis is a very simple problem that many of us learned how to do only a few hours in to basic R. But since I’ve been learning tidyverse package, I figured why not do it tidy-fashion?\nLet’s give it a try.\nHere is the question:\n I have the following sequence:\nS\u0026lt;-seq(1,90, by=3)\nI make a matrix that is the following:\nmatrix(S, nrow = 6, ncol = 5)\nNow I am trying to do the following:\nI want to calculate the means of the columns of the matrix and add them as a new row under the columns.\nNext I want to calculate the sum of the rows of the matrix and add them as a new column on the right of the matrix.\n Here is what their data looks like:\ndata_matrix = matrix(seq(1, 90, by = 3), nrow = 6, ncol = 5) data_matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 19 37 55 73 ## [2,] 4 22 40 58 76 ## [3,] 7 25 43 61 79 ## [4,] 10 28 46 64 82 ## [5,] 13 31 49 67 85 ## [6,] 16 34 52 70 88 They want to calculate the mean of each column and sum of each row.\nNow if you know some basic R this can very easily be achieved:\ndata_df = data.frame(data_matrix) data_df[,ncol(data_df)+1] = rowSums(data_df) data_df[nrow(data_df)+1,] = colMeans(data_df) head(data_df) ## X1 X2 X3 X4 X5 V6 ## 1 1 19 37 55 73 185 ## 2 4 22 40 58 76 200 ## 3 7 25 43 61 79 215 ## 4 10 28 46 64 82 230 ## 5 13 31 49 67 85 245 ## 6 16 34 52 70 88 260 But how can we do this “tidy” way? Turns out this is more complicated than I originally thought.\nFirst let’s breath some context into our data so we don’t get too bored:\nLet’s say we had an exam in a class of 50 students. The exam consists of 5 questions. TAs recorded the score of each question for each student on an excel sheet. The spreadsheet data has 5 columns, each for a question and 50 rows, each for a student. Now we want to calculate:\n1. Total score for each student (row sums)\n2. Class average for each question (col means)\nLet’s first creat this “spreadsheet”\nscores_df = data.frame(matrix(sample(1:20,250, replace = TRUE), nrow = 50, ncol = 5)) head(scores_df) ## X1 X2 X3 X4 X5 ## 1 19 13 7 9 16 ## 2 5 11 12 14 19 ## 3 5 11 6 8 20 ## 4 8 9 8 9 6 ## 5 3 14 4 7 6 ## 6 8 12 6 15 15 Here we have a data frame with 5 colmuns (5 questions) and 50 rows (50 students).\nNow the next step is to for tidy data: key-value pairs.\nFirst we need to identify the keys: in our dataset, each student-question pair uniquely identifies a value (score). So we can gather our date like this:\nlibrary(tidyverse) scores_tibble = gather(rownames_to_column(scores_df, var = \u0026quot;student\u0026quot;), question, score, -student) head(scores_tibble) ## student question score ## 1 1 X1 19 ## 2 2 X1 5 ## 3 3 X1 5 ## 4 4 X1 8 ## 5 5 X1 3 ## 6 6 X1 8 Here I did not unite() student and question into a single column so that later I can group by either student or question to calculate mean and sum.\nNow to calculate question means:\nsummarise(group_by(scores_tibble, question), mean = mean(score)) ## # A tibble: 5 x 2 ## question mean ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 X1 11 ## 2 X2 11.1 ## 3 X3 9.7 ## 4 X4 9.78 ## 5 X5 10.8 Student sum scores:\nsummarise(group_by(scores_tibble, student), total = sum(score)) ## # A tibble: 50 x 2 ## student total ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 1 64 ## 2 10 46 ## 3 11 78 ## 4 12 63 ## 5 13 50 ## 6 14 56 ## 7 15 44 ## 8 16 40 ## 9 17 64 ## 10 18 43 ## # … with 40 more rows Let’s get a single table with all information\nscores_complete = group_by(scores_tibble, question) %\u0026gt;% mutate(question_average = mean(score)) %\u0026gt;% group_by(student) %\u0026gt;% mutate(student_total = sum(score)) scores_complete ## # A tibble: 250 x 5 ## # Groups: student [50] ## student question score question_average student_total ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 X1 19 11 64 ## 2 2 X1 5 11 61 ## 3 3 X1 5 11 50 ## 4 4 X1 8 11 40 ## 5 5 X1 3 11 34 ## 6 6 X1 8 11 56 ## 7 7 X1 2 11 53 ## 8 8 X1 1 11 42 ## 9 9 X1 17 11 63 ## 10 10 X1 8 11 46 ## # … with 240 more rows With a tidy table we can easily calculate stats we want and add them to the original data frame:\nscores_df_complete = spread(scores_tibble, question, score) %\u0026gt;% left_join(summarise(group_by(scores_tibble, student), student_total = sum(score)), by = \u0026quot;student\u0026quot;) %\u0026gt;% bind_rows(spread(summarise(group_by(scores_tibble, question), question_mean = mean(score)), question, question_mean)) head(scores_df_complete) ## student X1 X2 X3 X4 X5 student_total ## 1 1 19 13 7 9 16 64 ## 2 10 8 20 4 1 13 46 ## 3 11 15 19 10 18 16 78 ## 4 12 6 14 13 20 10 63 ## 5 13 3 6 13 19 9 50 ## 6 14 10 19 10 15 2 56 tail(scores_df_complete) ## student X1 X2 X3 X4 X5 student_total ## 46 50 12 20.00 17.0 19.00 1.00 69 ## 47 6 8 12.00 6.0 15.00 15.00 56 ## 48 7 2 9.00 18.0 15.00 9.00 53 ## 49 8 1 16.00 11.0 3.00 11.00 42 ## 50 9 17 8.00 9.0 11.00 18.00 63 ## 51 \u0026lt;NA\u0026gt; 11 11.08 9.7 9.78 10.84 NA ","date":1552262400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552262400,"objectID":"9eaccf4c94c58e468784c9a82d2be8be","permalink":"/post/calculate-row-sums-and-col-means-with-tidyr/","publishdate":"2019-03-11T00:00:00Z","relpermalink":"/post/calculate-row-sums-and-col-means-with-tidyr/","section":"post","summary":"Recently Someone asked a question on reddit: Adding a row and a column that are means for a set matrix\nThis is a very simple problem that many of us learned how to do only a few hours in to basic R. But since I’ve been learning tidyverse package, I figured why not do it tidy-fashion?\nLet’s give it a try.\nHere is the question:\n I have the following sequence:","tags":["tidyverse","R"],"title":"Calculate row sums and col means with tidyR","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536476400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483257600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-08:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]