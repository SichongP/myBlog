---
title: Hypothesis testing and multiple test correction
author: Sichong Peng
date: '2019-07-11'
slug: fdr-correction
categories:
  - Statistics
tags:
  - R
  - Statistics
  - Population Genetics
image:
  caption: ''
  focal_point: ''
---



<div id="why-multiple-testing-correction" class="section level2">
<h2>Why multiple testing correction?</h2>
<p>Multiple testing refers to a practice where we apply the same statistical test <em>independantly</em> multiple times. This raises a problem that’s related to hypothesis testing: the more independant tests we do, the more inflated our false positive rate becomes. Here I’m gonna use a simple example to explain why multiple testing correction is crucial for hypothesis testing.</p>
<div id="an-example---are-you-being-cheated" class="section level3">
<h3>An example - Are you being cheated?</h3>
<p>Imagine that you’re playing a game where the host lets you flip a coin, if it’s a head, you get a point. If it’s a tail, the host gets a point. Whoever gets 20 points first wins the game. Does this sound like a fair game to you?</p>
<p>In this scenario, when we say the game is “fair”, we are implying that the coin is an average, normal coin, which gives you 50/50 chance for a head or a tail. But is this true?</p>
<p>Intuitively, we may find a coin that flips tail 10 times in a row quite suspicious. But what about 6 times in a row? 3 times? 9 out of 10 times? 3 out of 4 times? At which point do we decide to call it a fraud? Here is where hypothesis testing comes into play.</p>
<p>When we look at this scenario, we first assume that coin is normal, which gives us an expectation: the percentages of heads and tails are roughly 50% each. Then we flip the coin multiple times, observe the frequency of heads and tails and make a decision based on our observed data: does it comply with our expectation or does it deviate so much that we feel safe to say that our assumption of fairness was wrong?</p>
<p>The process above is essentially a hypothesis testing:
1. For a question, we formulate a null hypothesis and its alternative hypothesis
2. Based on our hypothesis, we draw some expectations of data assuming null hypothesis true
3. Carry out experiment, collect data and compare observed data to our expected data.
4. Make a decision - are we able to reject our null hypothesis or not?</p>
<p>In the coin flip example, if we record 1 everytime we get a head and -1 everytime we get a tail, after many times of flips, we would expect to get a mean score of 0. Let’s simulate this process in R:</p>
<pre class="r"><code>flips = sample(c(-1,1), 20, replace = TRUE, prob = c(0.5, 0.5))
flips</code></pre>
<pre><code>##  [1]  1  1  1 -1  1 -1  1 -1 -1 -1 -1 -1  1  1 -1  1 -1 -1 -1  1</code></pre>
<p>In the above code, by setting <code>prob=c(0.5, 0.5)</code>, we’re saying this is a fair coin. Now let’s see what the mean we got from this simulation is:</p>
<pre class="r"><code>mean(flips)</code></pre>
<pre><code>## [1] -0.1</code></pre>
<p>It’s not 0 as we expected it to be, even though we know we’re flipping a fair coin!
Now you probably know this is normal - The probability of getting 9 heads out of 20 flips is <span class="math inline">\(P_{9/20}=C_{20}^9(\frac{1}{2})^9(\frac{1}{2})^{11}=16%\)</span>, not a very low chance at all! This is called sampling variablity.</p>
<p>Therefore, just because observed data isn’t identical to our expected data, doesn’t mean we should reject our null hypothesis. Now how can we make this decision then?</p>
<p>Let’s see if we do the same simulation over and over, what the mean looks like:</p>
<pre class="r"><code>means=c()
for (i in c(1:1000))
{
  means = c(means, mean(sample(c(-1,1), 20, replace = TRUE, prob = c(0.5, 0.5))))
}
ggplot(data.frame(means)) + geom_histogram(aes(x=means), binwidth = 0.1)</code></pre>
<p><img src="/post/2019-07-11-fdr-correction_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>It looks like a normal distribution (sort of)! This tells us that if we do the same experiment over and over again, the mean should be follow a normal distribution and the majority of results should be near where the mean of this normal distribution (<span class="math inline">\(\mu=0\)</span>) is. Is it possible that we get a mean of 0.6 from a fair coin? <strong>Yes, possible but not likely!</strong></p>
<p>Of course, in reality we normally only run an experiment once and therefore, won’t get a distribution of means. But intuitively, we know if our mean is too far away from the center of this normal distribution, our true mean is probably not 0!</p>
<p>In practice, we compare our sample mean to a <em>t</em>-distribution. This is because our sample size is relatively small (20 flips) and there is some deviation from normal distribution when we calculate mean based on a small sample size. The shape of a <em>t</em>-distribution depends on sample size, or degree of freedom (df):</p>
<pre class="r"><code>x &lt;- seq(-4, 4, length=100)
hx &lt;- dnorm(x)

degf &lt;- c(1, 3, 8, 30)
colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;, &quot;gold&quot;, &quot;black&quot;)
labels &lt;- c(&quot;df=1&quot;, &quot;df=3&quot;, &quot;df=8&quot;, &quot;df=30&quot;, &quot;normal&quot;)

plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;,
  ylab=&quot;Density&quot;, main=&quot;Comparison of t Distributions&quot;)

for (i in 1:4){
  lines(x, dt(x,degf[i]), lwd=2, col=colors[i])
}

legend(&quot;topright&quot;, inset=.05, title=&quot;Distributions&quot;,
  labels, lwd=2, lty=c(1, 1, 1, 1, 2), col=colors)</code></pre>
<p><img src="/post/2019-07-11-fdr-correction_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</div>
